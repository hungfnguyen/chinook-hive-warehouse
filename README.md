# ğŸ§ Chinook Hive Warehouse

A full ETL pipeline project designed for practicing data engineering skills. The project extracts music store data from the Chinook SQL Server database, transforms it using Python, stages it as CSV, and loads it into Hive for analysis.

---

## ğŸ” ETL Pipeline Overview

```
SQL Server (Chinook)
    â”‚
    â””â”€â”€â–º Python ETL
            â”‚
            â”œâ”€ Extract raw data via pyodbc / SQLAlchemy
            â”œâ”€ Transform with business logic (fact & dimension tables)
            â””â”€ Save as CSV Stage files
                â”‚
                â””â”€â”€â–º Load into Hive (data warehouse)
                          â”‚
                          â””â”€â”€â–º Analyze with Python + ML (scikit-learn, matplotlib)
```

---

## ğŸ§± 1. Data Source

- **Chinook Database** (SQL Server)  
- Tables used:
  - `Customer`, `Invoice`, `InvoiceLine`  
  - `Artist`, `Album`, `Track`, `Genre`  
- Goal:
  - Analyze customer behavior, revenue trends, top artists/genres.

---

## ğŸ”§ 2. Data Processing & Staging

- **Tools:** Python, Pandas, pyodbc / SQLAlchemy  
- **Steps:**
  - Define business processes: purchase behavior, music sales  
  - Build star schema:
    - Dimension tables: `dim_customer`, `dim_artist`, `dim_album`, `dim_track`, `dim_genre`  
    - Fact table: `fact_sales`
  - Stage data as `.csv` files for manual inspection and Hive loading

---

## ğŸ 3. Hive Data Warehouse

- **Tools:** Apache Hive, PyHive (or Beeline)  
- **Steps:**
  - Create Hive schema (star schema format)  
  - Define tables for each dimension and fact  
  - Load CSV stage files into Hive:
    - Using `LOAD DATA INPATH` or Python + PyHive `INSERT INTO`

---

## ğŸ“Š 4. Data Analysis

- **Tools:** scikit-learn, matplotlib, seaborn  
- **Possible Analysis:**
  - ğŸ“Œ Customer segmentation (clustering by revenue, location, frequency)  
  - ğŸ“ˆ Revenue forecasting (monthly/quarterly using Linear Regression / ARIMA)  
  - ğŸ“Š Top artists and genres  
  - ğŸ“… Time-based sales analysis

---

## âš™ï¸ Technologies Used

- SQL Server (Chinook DB)  
- Python (Pandas, SQLAlchemy, PyHive)  
- Hive + HiveServer2 + Metastore (MariaDB)  
- Docker Compose  
- Scikit-learn, Matplotlib, Seaborn

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ chinook_etl/
â”‚   â”œâ”€â”€ extract/         # SQL Server connectors and queries
â”‚   â”œâ”€â”€ transform/       # Business logic and data modeling
â”‚   â””â”€â”€ stage/           # CSV outputs (dim/fact tables)
â”œâ”€â”€ hive/
â”‚   â”œâ”€â”€ conf/            # hive-site.xml and table schemas
â”‚   â”œâ”€â”€ hiveserver2/     # Custom Dockerfile with telnet, ping, etc.
â”‚   â””â”€â”€ docker-compose.yaml
â”œâ”€â”€ analysis/            # Python notebooks or scripts for ML
â”œâ”€â”€ README.md
â””â”€â”€ .env
```

---

## ğŸš€ Getting Started

1. **Start the Hive + Metastore environment:**

```bash
cd hive
docker-compose up -d
```

2. **Run ETL pipeline using Python:**

```bash
cd chinook_etl
python main.py
```

3. **Analyze data in Hive using Beeline or PyHive**

---

## ğŸ“Œ Author

Made with â¤ï¸ by **[hungfnguyen]** as a portfolio project for Data Engineer internships.
